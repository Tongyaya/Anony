{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs: 96\n",
      "Number of PROCESSs: 96\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "NUM_CPU = len(os.sched_getaffinity(0))\n",
    "print(f'Number of CPUs: {NUM_CPU}')\n",
    "NUM_THREADS = 1\n",
    "os.environ[\"OMP_NUM_THREADS\"]     = str(NUM_THREADS)\n",
    "os.environ[\"MKL_NUM_THREADS\"]     = str(NUM_THREADS)\n",
    "os.environ[\"OPENBIAS_NUM_THREADS\"] = str(NUM_THREADS)\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(NUM_THREADS)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"]  = str(NUM_THREADS)\n",
    "\n",
    "NUM_PROCESS = int(NUM_CPU // NUM_THREADS) \n",
    "print(f'Number of PROCESSs: {NUM_PROCESS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from copy import deepcopy\n",
    "np.set_printoptions(suppress=True)\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def beta_true_gen(N,p,K,gamma):\n",
    "    '''\n",
    "    Generate beta_true\n",
    "    '''\n",
    "    np.random.seed(0)  \n",
    "    beta_true = np.zeros([p+1,K-1])\n",
    "    for k in range(K-1):\n",
    "        beta_true[:,k] = np.random.normal(size = [p+1,]) \n",
    "        beta_true[:,k] = beta_true[:,k]/np.linalg.norm(beta_true[:,k])\n",
    "    beta_true[0,:] = beta_true[0,:] + gamma*np.log(N)\n",
    "    return beta_true\n",
    "\n",
    "def X_gen(N, p=5, rho=0.5):\n",
    "    '''\n",
    "    Generate features X\n",
    "    '''\n",
    "    mean = np.zeros(p)\n",
    "    cov = np.zeros([p,p])\n",
    "    for i in range(p):\n",
    "        for j in range(i,p):\n",
    "            cov[i,j]=rho**(np.abs(i-j))\n",
    "    cov = cov+cov.T-np.eye(p)        \n",
    "    X = np.random.multivariate_normal(mean, cov, (N,)) \n",
    "    return(X)\n",
    "\n",
    "def get_onehot(y,baseclass = None):\n",
    "    '''One-hot'''\n",
    "    idx = np.squeeze(y)\n",
    "    ss = len(y)\n",
    "    nclass = len(np.unique(y))\n",
    "    z = np.zeros([ss,nclass])\n",
    "    z[np.arange(ss),idx] = 1  \n",
    "    ls_class = list(np.arange(nclass))\n",
    "    if baseclass is None:\n",
    "        baseclass = K-1\n",
    "    _ = ls_class.pop(baseclass)\n",
    "    return z[:,ls_class]\n",
    "\n",
    "\n",
    "def gd_multilogistic_opt_ic(x,y,K0_pval,baseclass, alpha): \n",
    "    '''GMLE (GD algorithm)'''\n",
    "    ss,ncov = x.shape  \n",
    "    K = len(np.unique(y)) \n",
    "    y = get_onehot(y,baseclass) \n",
    "    dist = 1.0; niter = 0\n",
    "    beta0 = np.zeros([ncov*(K-1),1]) \n",
    "    alpha0 = np.log((1/K0_pval-1)/(K-1))\n",
    "    beta0[np.arange(K-1)*ncov] = alpha0\n",
    "    while (dist>1.0e-6) & (niter<1000):\n",
    "        niter=niter+1\n",
    "        beta0mat = (beta0.reshape([(K-1),ncov]).T)*1.0\n",
    "        link_mu = x@beta0mat  \n",
    "        prob = np.exp(link_mu);prob=prob/(1+np.sum(prob,axis = 1,keepdims=True))\n",
    "        resid = y-prob\n",
    "        D1=((x.T@resid/ss).T).reshape([-1,1])\n",
    "        beta1 = beta0 + alpha * D1\n",
    "        assert beta1.shape==(ncov*(K-1),1),'shape is wrong'\n",
    "        dist = np.mean(np.abs(beta1-beta0))\n",
    "        beta0 = beta1\n",
    "    return beta0.reshape([(K-1),ncov]).T, dist, niter \n",
    "\n",
    "\n",
    "def mle_logistic_cpu_ic(k):\n",
    "    '''PMLE'''\n",
    "    np.random.seed(k)\n",
    "    # Subsample\n",
    "    idx_k = np.where(Y==k)[0] \n",
    "    idx_k = np.concatenate((idx_0,idx_k)) \n",
    "    x = X[idx_k]; y = Y[idx_k]\n",
    "    # Optimization\n",
    "    ss,ncov = x.shape  \n",
    "    y = y.reshape(ss,1)\n",
    "    y = 1*(y!=0) \n",
    "    dist=1.0\n",
    "    niter=0\n",
    "    beta0 = np.zeros([ncov,1])\n",
    "    alpha0 = np.log(1/K0_pval-1)\n",
    "    beta0[0] = alpha0\n",
    "    \n",
    "    while (dist>1.0e-6) & (niter<50):\n",
    "        niter=niter+1\n",
    "        link_mu = x@beta0  \n",
    "        prob = np.exp(link_mu);prob=prob/(1+prob)\n",
    "        resid=y-prob\n",
    "        D1=x.T@resid/ss  # The first-order derivative\n",
    "        weight = np.sqrt(prob*(1-prob))\n",
    "        wx  = weight*x \n",
    "        del weight\n",
    "        D2=wx.T@wx/ss+1.0e-6*np.eye(ncov)  # The second-order derivative\n",
    "        del wx \n",
    "        step=np.linalg.inv(D2)@D1\n",
    "        beta1=beta0+step\n",
    "        assert beta1.shape==(ncov,1),'shape is wrong'\n",
    "        dist=np.mean(np.abs(beta1-beta0))\n",
    "        beta0=beta1\n",
    "    return beta1.reshape([ncov,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMLE(GD): 523.11s\n",
      "PMLE: 87.25s\n",
      "epoch 0: 624.19s\n",
      "GMLE(GD): 520.78s\n",
      "PMLE: 86.38s\n",
      "GMLE(GD): 514.15s\n",
      "PMLE: 89.37s\n",
      "GMLE(GD): 525.66s\n",
      "PMLE: 86.35s\n",
      "GMLE(GD): 513.02s\n",
      "PMLE: 88.36s\n",
      "GMLE(GD): 517.69s\n",
      "PMLE: 83.38s\n",
      "GMLE(GD): 525.66s\n",
      "PMLE: 88.87s\n",
      "GMLE(GD): 519.45s\n",
      "PMLE: 84.52s\n",
      "GMLE(GD): 519.34s\n",
      "PMLE: 88.98s\n",
      "GMLE(GD): 510.35s\n",
      "PMLE: 85.8s\n",
      "GMLE(GD): 516.94s\n",
      "PMLE: 86.79s\n",
      "GMLE(GD): 515.91s\n",
      "PMLE: 86.94s\n",
      "GMLE(GD): 519.27s\n",
      "PMLE: 87.75s\n",
      "GMLE(GD): 516.74s\n",
      "PMLE: 86.33s\n",
      "GMLE(GD): 514.99s\n",
      "PMLE: 86.34s\n",
      "GMLE(GD): 508.76s\n",
      "PMLE: 86.34s\n",
      "GMLE(GD): 500.96s\n",
      "PMLE: 87.27s\n",
      "GMLE(GD): 504.27s\n",
      "PMLE: 84.1s\n",
      "GMLE(GD): 504.52s\n",
      "PMLE: 86.93s\n",
      "GMLE(GD): 506.16s\n",
      "PMLE: 88.87s\n",
      "GMLE(GD): 518.2s\n",
      "PMLE: 85.83s\n",
      "GMLE(GD): 518.08s\n",
      "PMLE: 87.53s\n",
      "GMLE(GD): 517.88s\n",
      "PMLE: 84.48s\n",
      "GMLE(GD): 511.24s\n",
      "PMLE: 86.34s\n",
      "GMLE(GD): 497.23s\n",
      "PMLE: 87.06s\n",
      "GMLE(GD): 498.74s\n",
      "PMLE: 84.6s\n",
      "epoch 25: 597.14s\n",
      "GMLE(GD): 510.31s\n",
      "PMLE: 84.62s\n",
      "GMLE(GD): 501.12s\n",
      "PMLE: 85.02s\n",
      "GMLE(GD): 505.6s\n",
      "PMLE: 84.5s\n",
      "GMLE(GD): 506.48s\n",
      "PMLE: 86.24s\n",
      "GMLE(GD): 507.38s\n",
      "PMLE: 84.79s\n",
      "GMLE(GD): 504.13s\n",
      "PMLE: 85.14s\n",
      "GMLE(GD): 499.73s\n",
      "PMLE: 84.73s\n",
      "GMLE(GD): 507.58s\n",
      "PMLE: 86.03s\n",
      "GMLE(GD): 506.43s\n",
      "PMLE: 86.03s\n",
      "GMLE(GD): 503.15s\n",
      "PMLE: 83.84s\n",
      "GMLE(GD): 501.78s\n",
      "PMLE: 86.03s\n",
      "GMLE(GD): 503.16s\n",
      "PMLE: 88.93s\n",
      "GMLE(GD): 531.41s\n",
      "PMLE: 85.11s\n",
      "GMLE(GD): 514.96s\n",
      "PMLE: 88.44s\n",
      "GMLE(GD): 529.82s\n",
      "PMLE: 84.57s\n",
      "GMLE(GD): 523.84s\n",
      "PMLE: 87.04s\n",
      "GMLE(GD): 505.29s\n",
      "PMLE: 88.44s\n",
      "GMLE(GD): 503.99s\n",
      "PMLE: 84.93s\n",
      "GMLE(GD): 501.02s\n",
      "PMLE: 88.06s\n",
      "GMLE(GD): 507.95s\n",
      "PMLE: 87.76s\n",
      "GMLE(GD): 500.84s\n",
      "PMLE: 87.6s\n",
      "GMLE(GD): 500.71s\n",
      "PMLE: 89.71s\n",
      "GMLE(GD): 497.38s\n",
      "PMLE: 86.43s\n",
      "GMLE(GD): 497.46s\n",
      "PMLE: 83.52s\n",
      "GMLE(GD): 497.52s\n",
      "PMLE: 87.54s\n",
      "epoch 50: 598.82s\n",
      "GMLE(GD): 497.54s\n",
      "PMLE: 88.01s\n",
      "GMLE(GD): 497.44s\n",
      "PMLE: 85.92s\n",
      "GMLE(GD): 502.73s\n",
      "PMLE: 83.91s\n",
      "GMLE(GD): 506.73s\n",
      "PMLE: 88.55s\n",
      "GMLE(GD): 499.16s\n",
      "PMLE: 84.69s\n",
      "GMLE(GD): 502.96s\n",
      "PMLE: 85.0s\n",
      "GMLE(GD): 498.03s\n",
      "PMLE: 86.67s\n",
      "GMLE(GD): 506.12s\n",
      "PMLE: 87.02s\n",
      "GMLE(GD): 499.44s\n",
      "PMLE: 84.22s\n",
      "GMLE(GD): 507.54s\n",
      "PMLE: 83.93s\n",
      "GMLE(GD): 503.43s\n",
      "PMLE: 88.07s\n",
      "GMLE(GD): 505.86s\n",
      "PMLE: 87.19s\n",
      "GMLE(GD): 498.37s\n",
      "PMLE: 85.5s\n",
      "GMLE(GD): 506.42s\n",
      "PMLE: 85.64s\n",
      "GMLE(GD): 505.37s\n",
      "PMLE: 85.81s\n",
      "GMLE(GD): 509.94s\n",
      "PMLE: 85.51s\n",
      "GMLE(GD): 519.61s\n",
      "PMLE: 84.62s\n",
      "GMLE(GD): 516.69s\n",
      "PMLE: 86.74s\n",
      "GMLE(GD): 510.95s\n",
      "PMLE: 83.59s\n",
      "GMLE(GD): 516.56s\n",
      "PMLE: 85.72s\n",
      "GMLE(GD): 508.29s\n",
      "PMLE: 85.73s\n",
      "GMLE(GD): 513.77s\n",
      "PMLE: 84.41s\n",
      "GMLE(GD): 511.9s\n",
      "PMLE: 87.54s\n",
      "GMLE(GD): 510.79s\n",
      "PMLE: 86.47s\n",
      "GMLE(GD): 505.73s\n",
      "PMLE: 85.34s\n",
      "epoch 75: 604.86s\n",
      "GMLE(GD): 515.36s\n",
      "PMLE: 88.65s\n",
      "GMLE(GD): 513.57s\n",
      "PMLE: 85.08s\n",
      "GMLE(GD): 514.76s\n",
      "PMLE: 86.54s\n",
      "GMLE(GD): 517.83s\n",
      "PMLE: 85.52s\n",
      "GMLE(GD): 516.65s\n",
      "PMLE: 88.06s\n",
      "GMLE(GD): 507.67s\n",
      "PMLE: 84.53s\n",
      "GMLE(GD): 514.27s\n",
      "PMLE: 87.81s\n",
      "GMLE(GD): 502.87s\n",
      "PMLE: 88.12s\n",
      "GMLE(GD): 513.79s\n",
      "PMLE: 85.85s\n",
      "GMLE(GD): 506.64s\n",
      "PMLE: 85.07s\n",
      "GMLE(GD): 506.37s\n",
      "PMLE: 87.74s\n",
      "GMLE(GD): 516.64s\n",
      "PMLE: 87.13s\n",
      "GMLE(GD): 504.14s\n",
      "PMLE: 88.06s\n",
      "GMLE(GD): 503.14s\n",
      "PMLE: 86.7s\n",
      "GMLE(GD): 500.57s\n",
      "PMLE: 86.45s\n",
      "GMLE(GD): 506.24s\n",
      "PMLE: 88.15s\n",
      "GMLE(GD): 506.62s\n",
      "PMLE: 88.91s\n",
      "GMLE(GD): 501.75s\n",
      "PMLE: 84.71s\n",
      "GMLE(GD): 501.01s\n",
      "PMLE: 86.72s\n",
      "GMLE(GD): 501.35s\n",
      "PMLE: 87.24s\n",
      "GMLE(GD): 504.36s\n",
      "PMLE: 83.57s\n",
      "GMLE(GD): 506.12s\n",
      "PMLE: 87.56s\n",
      "GMLE(GD): 514.67s\n",
      "PMLE: 85.84s\n",
      "GMLE(GD): 519.17s\n",
      "PMLE: 86.76s\n",
      "\n",
      "60917.6334s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "N = 10**5, 2*10**5, 5*10**5\n",
    "'''\n",
    "N = 10**5     # Sample size\n",
    "p = 500       # Feature dimension\n",
    "K = 51        # Number of Classes\n",
    "gamma = -0.5  # Rareness\n",
    "nsimu = 100\n",
    "\n",
    "alpha = 40\n",
    "baseclass = 0\n",
    "K0_pval = 0.9\n",
    "a = 25\n",
    "\n",
    "t_gd = np.zeros(nsimu)\n",
    "t_pmle = np.zeros(nsimu)\n",
    "\n",
    "beta_hat = np.zeros([nsimu,p+1,(K-1)])\n",
    "beta_hat_gd = np.zeros([nsimu,p+1,(K-1)])\n",
    "dist = np.zeros([nsimu]); dist_gd = 0*dist\n",
    "niter = np.zeros([nsimu]); niter_gd = 0*niter\n",
    "num_pos_idxs = np.zeros([nsimu,K])\n",
    "beta_true = beta_true_gen(N,p,K,-0.5) \n",
    "\n",
    "t3 = time.time()\n",
    "for b in range(nsimu):\n",
    "    if (b % a==0): t1 = time.time()\n",
    "    \n",
    "    np.random.seed(b)\n",
    "    X = X_gen(N,p)\n",
    "    X = np.hstack([np.ones([N,1]),X]) \n",
    "    prob = np.exp(X@beta_true) \n",
    "    prob = np.hstack([np.ones([N,1]),prob])   \n",
    "    prob = prob/np.sum(prob,1).reshape([N,1]) \n",
    "    prob = np.cumsum(prob,1) \n",
    "    Y = (np.random.uniform(size = [N,1])<prob).astype(np.int16) \n",
    "    Y = np.argmax(Y,1) \n",
    "    idx_0 = np.where(Y==0)[0] \n",
    "    \n",
    "    cls = np.zeros([N,K])\n",
    "    cls[np.arange(N),Y] = 1 \n",
    "    num_pos_idxs[b] =  np.sum(cls,0) \n",
    "    del cls\n",
    "    \n",
    "    ## GMLE: GD\n",
    "    t5 = time.time()\n",
    "    beta_hat_gd[b], dist_gd[b], niter_gd[b] = gd_multilogistic_opt_ic(X,Y,K0_pval,baseclass,alpha)\n",
    "    t6 = time.time(); t_gd[b] = t6-t5\n",
    "    print(f'GMLE(GD): {np.round(t6-t5,2)}s')\n",
    "    \n",
    "    ## PMLE: NR\n",
    "    np.random.seed(b)\n",
    "    t5 = time.time()\n",
    "    with Pool(K-1) as pool:\n",
    "        beta_hat[b] = np.array(pool.map(mle_logistic_cpu_ic, [k for k in range(1,K)])).T\n",
    "    t6 = time.time(); t_pmle[b] = t6-t5\n",
    "    print(f'PMLE: {np.round(t6-t5,2)}s')\n",
    "    \n",
    "    if (b % a==0): \n",
    "        t2 = time.time(); print('epoch {}: {}s'.format(b,np.round(t2-t1, 2)))\n",
    "\n",
    "t4 = time.time(); print(f'\\n{np.round(t4-t3,4)}s')\n",
    "\n",
    "# np.savez(f'/mnt/multi-class_simu/simulation/result/simu1/N{int(N/10**5)}_K{K}_p{p}_K0_pval[{K0_pval}].npz', \n",
    "#          beta_pmle=beta_hat, beta_gd=beta_hat_gd, beta_true=beta_true,\n",
    "#          t_pmle = t_pmle, t_gd = t_gd, num_pos_idxs=num_pos_idxs,\n",
    "#          niter_gd = niter_gd, dist_gd = dist_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
