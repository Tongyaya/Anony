{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "from utils import *\n",
    "from model import LogisticRegression\n",
    "from prepare_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def My_loss(labels, logits, samples_per_cls, no_of_classes, loss_type, beta, gamma): \n",
    "    \n",
    "    if loss_type == \"CSL\":\n",
    "        assert beta == 1,\"Cost_sensitive should set beta=1\"\n",
    "        \n",
    "        prob_per_cls = np.array(samples_per_cls)/np.sum(np.array(samples_per_cls)) \n",
    "        weights = 1 / prob_per_cls\n",
    "        weights = torch.tensor(weights / np.sum(weights) * no_of_classes).float()\n",
    "        \n",
    "        loss = F.cross_entropy(input = logits, target = labels, weight = weights)\n",
    "    \n",
    "    elif loss_type == \"CBL\":   # CB_loss\n",
    "        assert beta != 0,\"class balanced loss shouldn't set beta=0\"\n",
    "        \n",
    "        effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
    "        weights = (1.0 - beta) / np.array(effective_num)\n",
    "        weights = torch.tensor(weights / np.sum(weights) * no_of_classes).float()\n",
    "        \n",
    "        loss = F.cross_entropy(input = logits, target = labels, weight = weights)\n",
    "    \n",
    "    elif loss_type == \"FL\":   # CB_loss\n",
    "        assert beta == 0,\"focal loss should set beta=0\"\n",
    "        \n",
    "        \n",
    "        labels_one_hot = F.one_hot(labels, no_of_classes).float()\n",
    "        \n",
    "        alpha = 0.25\n",
    "        weights = alpha * torch.ones(labels.shape[0], no_of_classes)\n",
    "        BCLoss = F.binary_cross_entropy_with_logits(input = logits, target = labels_one_hot,reduction = \"none\")\n",
    "        modulator = torch.exp(-gamma * labels_one_hot * logits - gamma * torch.log(1 + torch.exp(-1.0 * logits)))\n",
    "        floss = modulator * BCLoss\n",
    "        weighted_loss = weights * floss\n",
    "        loss = torch.sum(weighted_loss)\n",
    "        loss /= torch.sum(labels_one_hot)\n",
    "    elif loss_type == \"RDS\":\n",
    "        loss = F.cross_entropy(input = logits, target = labels)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/icml_2025_multi-class/real data/code_submit/utils.py:125: RuntimeWarning: invalid value encountered in true_divide\n",
      "  X_std = (X_non-Xmin)/(Xmax-Xmin); X_std[:,idx] = 0\n"
     ]
    }
   ],
   "source": [
    "## Loss type selection\n",
    "\n",
    "# loss_type = \"CSL\"\n",
    "# loss_type = \"FL\"\n",
    "# loss_type = \"CBL\"\n",
    "loss_type = \"RDS\"\n",
    "params = Params(loss_type)\n",
    "\n",
    "## Load data\n",
    "traindata = np.load(f'/mnt/multi-class_simu/real data/Audi/data/512_traindata_8cls.npz')\n",
    "testdata = np.load(f'/mnt/multi-class_simu/real data/Audi/data/512_testdata_8cls.npz')\n",
    "\n",
    "samples_per_cls,dataloader,Ns0,X1_torch,Y1_torch,X1,Y1 = mkdata(loss_type,traindata,testdata,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/90, Loss: 0.8287117248773574\n",
      "Epoch 2/90, Loss: 0.5741238868236542\n",
      "Epoch 3/90, Loss: 0.5007968431711197\n",
      "Epoch 4/90, Loss: 0.45617625415325164\n",
      "Epoch 5/90, Loss: 0.42292893171310425\n",
      "Epoch 6/90, Loss: 0.3984169238805771\n",
      "Epoch 7/90, Loss: 0.3783349734544754\n",
      "Epoch 8/90, Loss: 0.36161125004291533\n",
      "Epoch 9/90, Loss: 0.34646953642368317\n",
      "Epoch 10/90, Loss: 0.3339306643605232\n",
      "Epoch 11/90, Loss: 0.32292487531900405\n",
      "Epoch 12/90, Loss: 0.3124754500389099\n",
      "Epoch 13/90, Loss: 0.30334572821855543\n",
      "Epoch 14/90, Loss: 0.2951361933350563\n",
      "Epoch 15/90, Loss: 0.2875497755408287\n",
      "Epoch 16/90, Loss: 0.28055752605199813\n",
      "Epoch 17/90, Loss: 0.27418875962495803\n",
      "Epoch 18/90, Loss: 0.2684985277056694\n",
      "Epoch 19/90, Loss: 0.2621381512284279\n",
      "Epoch 20/90, Loss: 0.25747287154197696\n",
      "Epoch 21/90, Loss: 0.2523703783750534\n",
      "Epoch 22/90, Loss: 0.24774535447359086\n",
      "Epoch 23/90, Loss: 0.24381056070327758\n",
      "Epoch 24/90, Loss: 0.23900071829557418\n",
      "Epoch 25/90, Loss: 0.23519689440727234\n",
      "Epoch 26/90, Loss: 0.23171984404325485\n",
      "Epoch 27/90, Loss: 0.2283466014266014\n",
      "Epoch 28/90, Loss: 0.22505681365728378\n",
      "Epoch 29/90, Loss: 0.22203023374080658\n",
      "Epoch 30/90, Loss: 0.21903387933969498\n",
      "Epoch 31/90, Loss: 0.21726391434669495\n",
      "Epoch 32/90, Loss: 0.2167181208729744\n",
      "Epoch 33/90, Loss: 0.21631432503461837\n",
      "Epoch 34/90, Loss: 0.21614437699317932\n",
      "Epoch 35/90, Loss: 0.2164793123304844\n",
      "Epoch 36/90, Loss: 0.21591544419527053\n",
      "Epoch 37/90, Loss: 0.21532040894031523\n",
      "Epoch 38/90, Loss: 0.21501926332712173\n",
      "Epoch 39/90, Loss: 0.21459070593118668\n",
      "Epoch 40/90, Loss: 0.21402978926897048\n",
      "Epoch 41/90, Loss: 0.21380273669958114\n",
      "Epoch 42/90, Loss: 0.21391062408685685\n",
      "Epoch 43/90, Loss: 0.2139565095305443\n",
      "Epoch 44/90, Loss: 0.21344182312488555\n",
      "Epoch 45/90, Loss: 0.2131283873319626\n",
      "Epoch 46/90, Loss: 0.21267153948545456\n",
      "Epoch 47/90, Loss: 0.2127649000287056\n",
      "Epoch 48/90, Loss: 0.21210966408252716\n",
      "Epoch 49/90, Loss: 0.21187207221984863\n",
      "Epoch 50/90, Loss: 0.21164606034755706\n",
      "Epoch 51/90, Loss: 0.21160992681980134\n",
      "Epoch 52/90, Loss: 0.21127031475305558\n",
      "Epoch 53/90, Loss: 0.21100551426410674\n",
      "Epoch 54/90, Loss: 0.21045923486351967\n",
      "Epoch 55/90, Loss: 0.21020228907465935\n",
      "Epoch 56/90, Loss: 0.21004276752471923\n",
      "Epoch 57/90, Loss: 0.20990098714828492\n",
      "Epoch 58/90, Loss: 0.20968706652522087\n",
      "Epoch 59/90, Loss: 0.20912206739187242\n",
      "Epoch 60/90, Loss: 0.20883231103420258\n",
      "Epoch 61/90, Loss: 0.20880803257226943\n",
      "Epoch 62/90, Loss: 0.2087957462668419\n",
      "Epoch 63/90, Loss: 0.20854484796524048\n",
      "Epoch 64/90, Loss: 0.2088172283768654\n",
      "Epoch 65/90, Loss: 0.20884513944387437\n",
      "Epoch 66/90, Loss: 0.20877863645553588\n",
      "Epoch 67/90, Loss: 0.20853532403707503\n",
      "Epoch 68/90, Loss: 0.2084887996315956\n",
      "Epoch 69/90, Loss: 0.20861794024705888\n",
      "Epoch 70/90, Loss: 0.20922408491373062\n",
      "Epoch 71/90, Loss: 0.20839383870363234\n",
      "Epoch 72/90, Loss: 0.20874787658452987\n",
      "Epoch 73/90, Loss: 0.20898814767599105\n",
      "Epoch 74/90, Loss: 0.20848382294178008\n",
      "Epoch 75/90, Loss: 0.20857696622610092\n",
      "Epoch 76/90, Loss: 0.2083446952700615\n",
      "Epoch 77/90, Loss: 0.20842996895313262\n",
      "Epoch 78/90, Loss: 0.20857021242380142\n",
      "Epoch 79/90, Loss: 0.20870010316371918\n",
      "Epoch 80/90, Loss: 0.20833245754241944\n",
      "Epoch 81/90, Loss: 0.20849625796079635\n",
      "Epoch 82/90, Loss: 0.20849136888980865\n",
      "Epoch 83/90, Loss: 0.20838257640600205\n",
      "Epoch 84/90, Loss: 0.20850033879280092\n",
      "Epoch 85/90, Loss: 0.2086079801619053\n",
      "Epoch 86/90, Loss: 0.2082425582408905\n",
      "Epoch 87/90, Loss: 0.2079080620408058\n",
      "Epoch 88/90, Loss: 0.20869758009910583\n",
      "Epoch 89/90, Loss: 0.20814103960990907\n",
      "Epoch 90/90, Loss: 0.20810147881507873\n",
      "End: 13.0278s\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "model = LogisticRegression(params[\"input_size\"], params[\"num_classes\"])\n",
    "optimizer = optim.SGD(model.parameters(), lr=params[\"lr\"])\n",
    "t1 = time.time()\n",
    "\n",
    "for epoch in range(params[\"num_epochs\"]):\n",
    "    running_loss = 0.0\n",
    "    if epoch == params[\"warm_up1\"]:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= params[\"multi_lr\"]\n",
    "    if epoch == params[\"warm_up2\"]:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= params[\"multi_lr\"]\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        outputs = model(inputs)\n",
    "        loss = My_loss(labels, outputs, samples_per_cls, params[\"num_classes\"],loss_type, params[\"beta\"], params[\"gamma\"])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}/{params[\"num_epochs\"]}, Loss: {running_loss / len(dataloader)}')\n",
    "t2 = time.time(); print(f'End: {np.round(t2-t1,4)}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model,'focal_loss.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 cls acc = 0.76,auc = 0.996\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "\n",
    "outputs = model(X1_torch)\n",
    "prob_hat = F.softmax(outputs, dim=1)\n",
    "_, Yhat = torch.max(outputs.data, 1)\n",
    "prob_hat = prob_hat.detach().numpy()\n",
    "Yhat = Yhat.detach().numpy() \n",
    "\n",
    "w = model.linear.weight.detach().numpy() # [K,p+1]\n",
    "b = model.linear.bias.detach().numpy() # [1,p+1]\n",
    "w = np.hstack((b.reshape([-1,1]),w))\n",
    "w_stand = w - w[0]\n",
    "beta_hat = w_stand[1:].T \n",
    "\n",
    "test_accw(np.concatenate((np.ones([X1.shape[0],1]),X1),1), Y1, Ns0, beta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
